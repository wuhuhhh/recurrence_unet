import torch
import torch.nn as nn


class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)


class UNetEncoder(nn.Module):
    def __init__(self, in_channels=3, features=[64, 128, 256, 512, 1024]):
        super(UNetEncoder, self).__init__()
        self.encoder_layers = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        for feature in features:
            self.encoder_layers.append(DoubleConv(in_channels, feature))
            in_channels = feature

    def forward(self, x):
        skip_connections = []
        for down in self.encoder_layers:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)
        encoder_output = x
        return encoder_output, skip_connections


class UNetDecoder(nn.Module):
    def __init__(self, features=[512, 256, 128, 64], bottleneck_channels=1024):
        super(UNetDecoder, self).__init__()
        self.decoder_layers = nn.ModuleList()
        self.up_convs = nn.ModuleList()

        # æ„å»ºä¸Šé‡‡æ ·å±‚ï¼šä»ç“¶é¢ˆå±‚é€šé“æ•°å¼€å§‹
        in_channels = bottleneck_channels
        for feature in features:
            self.up_convs.append(
                nn.ConvTranspose2d(
                    in_channels=in_channels,
                    out_channels=feature,
                    kernel_size=2,
                    stride=2
                )
            )
            # å…³é”®ä¿®æ­£ï¼šåŒå·ç§¯çš„è¾“å…¥é€šé“æ•° = feature + å¯¹åº”ç¼–ç å™¨å±‚çš„è¾“å‡ºé€šé“æ•°
            # å¯¹äºç¬¬ä¸€ä¸ªè§£ç å±‚ï¼š512 + 512 = 1024
            self.decoder_layers.append(
                DoubleConv(feature + feature, feature)  # ä¿®æ­£ï¼šfeature + feature
            )
            in_channels = feature

    def forward(self, x, skip_connections):
        # å…³é”®ä¿®æ­£ï¼šåªä½¿ç”¨å‰4ä¸ªè·³è·ƒè¿æ¥ï¼Œå»æ‰æœ€åä¸€ä¸ª
        skip_connections = skip_connections[:-1][::-1]  # å»æ‰æœ€åä¸€ä¸ªï¼Œç„¶ååè½¬

        for idx, (up_conv, double_conv) in enumerate(zip(self.up_convs, self.decoder_layers)):
            # ä¸Šé‡‡æ ·
            x = up_conv(x)

            # è·å–å¯¹åº”çš„è·³è·ƒè¿æ¥
            skip_connection = skip_connections[idx]

            # å°ºå¯¸å¯¹é½
            if x.shape[2:] != skip_connection.shape[2:]:
                target_height = skip_connection.shape[2]
                target_width = skip_connection.shape[3]
                x = torch.nn.functional.interpolate(
                    x,
                    size=(target_height, target_width),
                    mode='bilinear',
                    align_corners=True
                )

            # é€šé“æ‹¼æ¥
            concat_skip = torch.cat((skip_connection, x), dim=1)

            # åŒå·ç§¯
            x = double_conv(concat_skip)

        return x

class UNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512, 1024]):
        super(UNet, self).__init__()
        self.encoder = UNetEncoder(in_channels, features)
        decoder_features = features[:-1][::-1]  # [512, 256, 128, 64]
        self.decoder = UNetDecoder(
            features=decoder_features,
            bottleneck_channels=features[-1]  # 1024
        )
        self.final_conv = nn.Conv2d(decoder_features[-1], out_channels, kernel_size=1)

    def forward(self, x):
        bottleneck, skip_connections = self.encoder(x)
        x = self.decoder(bottleneck, skip_connections)
        return self.final_conv(x)


def test_decoder():
    print("=== æµ‹è¯•UNetè§£ç å™¨ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, channels, height, width = 1, 3, 512, 512
    x = torch.randn((batch_size, channels, height, width))
    print(f"è¾“å…¥å›¾åƒå°ºå¯¸: {x.shape}")

    # æµ‹è¯•ç¼–ç å™¨
    encoder = UNetEncoder(in_channels=3, features=[64, 128, 256, 512, 1024])
    encoder_output, skip_connections = encoder(x)

    print("\n=== ç¼–ç å™¨è¾“å‡º ===")
    print(f"ç“¶é¢ˆå±‚è¾“å‡ºå°ºå¯¸: {encoder_output.shape}")
    for i, skip in enumerate(skip_connections):
        print(f"Skip connection {i}: {skip.shape}")

    # æµ‹è¯•è§£ç å™¨
    print("\n=== è§£ç å™¨æµ‹è¯• ===")
    decoder_features = [512, 256, 128, 64]
    decoder = UNetDecoder(features=decoder_features, bottleneck_channels=1024)

    # è§£ç å™¨å‰å‘ä¼ æ’­
    decoder_output = decoder(encoder_output, skip_connections)
    print(f"è§£ç å™¨è¾“å‡ºå°ºå¯¸: {decoder_output.shape}")


def test_complete_unet():
    print("\n" + "=" * 50)
    print("=== æµ‹è¯•å®Œæ•´UNetæ¨¡å‹ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn((1, 3, 512, 512))
    print(f"è¾“å…¥å°ºå¯¸: {x.shape}")

    # åˆ›å»ºå®Œæ•´UNet
    model = UNet(in_channels=3, out_channels=1, features=[64, 128, 256, 512, 1024])

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(x)

    print(f"UNetè¾“å‡ºå°ºå¯¸: {output.shape}")

    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params:,}")

    return output


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    try:
        test_decoder()
        output = test_complete_unet()
        print("\nğŸ‰ UNetå®ç°æˆåŠŸï¼æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print(f"æœ€ç»ˆè¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()