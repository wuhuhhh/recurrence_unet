import torch
import torch.nn as nn


class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)


class UNetEncoder(nn.Module):
    def __init__(self, in_channels=3, features=[64, 128, 256, 512,1024]):
        super(UNetEncoder, self).__init__()
        self.in_channels = in_channels
        self.encoder_layers = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # æ·»åŠ åˆå§‹å·ç§¯å±‚
        self.initial_conv = DoubleConv(in_channels, features[0])  # 3â†’64

        # æ„å»ºencoder_layersï¼Œä»features[0]å¼€å§‹ä½œä¸ºè¾“å…¥é€šé“
        current_channels = features[0]  # 64
        for feature in features[1:]:  # ä»128å¼€å§‹
            self.encoder_layers.append(DoubleConv(current_channels, feature))
            current_channels = feature

    def forward(self, x):
        # ä½¿ç”¨åˆå§‹å·ç§¯
        x = self.initial_conv(x)  # [batch, 64, H, W]

        skip_connections = []
        for down in self.encoder_layers:
            skip_connections.append(x)  # ä¿å­˜å½“å‰ç‰¹å¾å›¾
            x = self.pool(x)  # ä¸‹é‡‡æ ·
            x = down(x)  # åŒå·ç§¯

        encoder_output = x
        return encoder_output, skip_connections
class UNetDecoder(nn.Module):
    def __init__(self, features=[512, 256, 128, 64], bottleneck_channels=1024):
        super(UNetDecoder, self).__init__()

        self.decoder_layers = nn.ModuleList()
        self.up_convs = nn.ModuleList()


        # æ„å»ºä¸Šé‡‡æ ·å±‚ï¼šä»ç“¶é¢ˆå±‚é€šé“æ•°å¼€å§‹
        in_channels = bottleneck_channels
        for feature in features:
            self.up_convs.append(
                nn.ConvTranspose2d(
                    in_channels=in_channels,
                    out_channels=feature,
                    kernel_size=2,
                    stride=2
                )
            )

            self.decoder_layers.append(
                DoubleConv(feature + feature, feature)
            )
            in_channels = feature

    def forward(self, x, skip_connections):
        # å…³é”®ä¿®æ­£ï¼šåªä½¿ç”¨å‰4ä¸ªè·³è·ƒè¿æ¥ï¼Œå»æ‰æœ€åä¸€ä¸ª
        skip_connections = skip_connections[::-1]  # å»æ‰æœ€åä¸€ä¸ªï¼Œç„¶ååè½¬

        for idx, (up_conv, double_conv) in enumerate(zip(self.up_convs, self.decoder_layers)):
            # ä¸Šé‡‡æ ·
            x = up_conv(x)

            # è·å–å¯¹åº”çš„è·³è·ƒè¿æ¥
            skip_connection = skip_connections[idx]

            # å°ºå¯¸å¯¹é½
            # if x.shape[2:] != skip_connection.shape[2:]:
            #     target_height = skip_connection.shape[2]
            #     target_width = skip_connection.shape[3]
            #     x = torch.nn.functional.interpolate(
            #         x,
            #         size=(target_height, target_width),
            #         mode='bilinear',
            #         align_corners=True
            #     )

            # é€šé“æ‹¼æ¥
            concat_skip = torch.cat((skip_connection, x), dim=1)

            # åŒå·ç§¯
            x = double_conv(concat_skip)

        return x

class UNet(nn.Module):
    def __init__(self, n_channels=3, n_classes=2, features=[64, 128, 256, 512,1024]):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.encoder = UNetEncoder(n_channels, features)
        decoder_features = features[:-1][::-1]  # [512, 256, 128, 64]
        self.decoder = UNetDecoder(
            features=decoder_features,
            bottleneck_channels=features[-1]
        )
        self.final_conv = nn.Conv2d(decoder_features[-1], n_classes, kernel_size=1)

    def forward(self, x):
        bottleneck, skip_connections = self.encoder(x)
        x = self.decoder(bottleneck, skip_connections)
        return self.final_conv(x)


def test_complete_unet():
    print("\n" + "=" * 50)
    print("=== æµ‹è¯•å®Œæ•´UNetæ¨¡å‹ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn((1, 3, 512, 512))
    print(f"è¾“å…¥å°ºå¯¸: {x.shape}")

    # åˆ›å»ºå®Œæ•´UNet
    model = UNet(n_channels=3, n_classes=2, features=[64, 128, 256, 512,1024])

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(x)

    print(f"UNetè¾“å‡ºå°ºå¯¸: {output.shape}")

    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params:,}")

    return output


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_complete_unet()
    # try:
    #     test_decoder()
    #     output = test_complete_unet()
    #     print("\nğŸ‰ UNetå®ç°æˆåŠŸï¼æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    #     print(f"æœ€ç»ˆè¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
    # except Exception as e:
    #     print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
    #     import traceback
    #
    #     traceback.print_exc()