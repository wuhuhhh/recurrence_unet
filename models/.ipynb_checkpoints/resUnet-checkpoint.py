import torch
import torch.nn as nn


class ResidualDoubleConv(nn.Module):
    """å¸¦æœ‰æ®‹å·®è¿æ¥çš„åŒå·ç§¯æ¨¡å—"""

    def __init__(self, in_channels, out_channels):
        super().__init__()

        # å¦‚æœè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒï¼Œéœ€è¦1x1å·ç§¯è°ƒæ•´é€šé“
        self.use_shortcut = (in_channels != out_channels)
        if self.use_shortcut:
            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
            self.bn_shortcut = nn.BatchNorm2d(out_channels)

        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x

        out = self.double_conv(x)

        # å¤„ç†shortcutè¿æ¥
        if self.use_shortcut:
            identity = self.bn_shortcut(self.shortcut(identity))

        out += identity  # æ®‹å·®è¿æ¥
        out = self.relu(out)

        return out


class ResidualUNetEncoder(nn.Module):
    """ä½¿ç”¨æ®‹å·®å—çš„ç¼–ç å™¨"""

    def __init__(self, in_channels=3, features=[64, 128, 256, 512, 1024]):
        super(ResidualUNetEncoder, self).__init__()
        self.encoder_layers = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # åˆå§‹å·ç§¯å±‚ä½¿ç”¨æ®‹å·®å—
        self.initial_conv = ResidualDoubleConv(in_channels, features[0])

        # æ„å»ºç¼–ç å±‚
        current_channels = features[0]
        for feature in features[1:]:
            self.encoder_layers.append(ResidualDoubleConv(current_channels, feature))
            current_channels = feature

    def forward(self, x):
        x = self.initial_conv(x)
        skip_connections = []

        for down in self.encoder_layers:
            skip_connections.append(x)
            x = self.pool(x)
            x = down(x)

        encoder_output = x
        return encoder_output, skip_connections


class ResidualUNetDecoder(nn.Module):
    """ä½¿ç”¨æ®‹å·®å—çš„è§£ç å™¨"""

    def __init__(self, features=[512, 256, 128, 64], bottleneck_channels=1024):
        super(ResidualUNetDecoder, self).__init__()
        self.decoder_layers = nn.ModuleList()
        self.up_convs = nn.ModuleList()

        in_channels = bottleneck_channels
        for feature in features:
            # ä¸Šé‡‡æ ·å±‚
            self.up_convs.append(
                nn.ConvTranspose2d(
                    in_channels=in_channels,
                    out_channels=feature,
                    kernel_size=2,
                    stride=2
                )
            )
            # è§£ç å™¨ä½¿ç”¨æ®‹å·®å—ï¼Œè¾“å…¥é€šé“æ˜¯ feature*2 (skip connection + ä¸Šé‡‡æ ·ç»“æœ)
            self.decoder_layers.append(
                ResidualDoubleConv(feature * 2, feature)
            )
            in_channels = feature

    def forward(self, x, skip_connections):
        skip_connections = skip_connections[::-1]  # åè½¬è·³è·ƒè¿æ¥

        for idx, (up_conv, residual_conv) in enumerate(zip(self.up_convs, self.decoder_layers)):
            x = up_conv(x)
            skip_connection = skip_connections[idx]

            # å°ºå¯¸å¯¹é½ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if x.shape != skip_connection.shape:
                x = torch.nn.functional.interpolate(
                    x, size=skip_connection.shape[2:], mode='bilinear', align_corners=True
                )

            # é€šé“æ‹¼æ¥
            concat_skip = torch.cat((skip_connection, x), dim=1)
            x = residual_conv(concat_skip)

        return x


class ResidualUNet(nn.Module):
    """å®Œæ•´çš„æ®‹å·®UNet"""

    def __init__(self, n_channels=3, n_classes=2, features=[64, 128, 256, 512, 1024]):
        super(ResidualUNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes

        self.encoder = ResidualUNetEncoder(n_channels, features)
        decoder_features = features[:-1][::-1]  # [512, 256, 128, 64]
        self.decoder = ResidualUNetDecoder(
            features=decoder_features,
            bottleneck_channels=features[-1]
        )
        self.final_conv = nn.Conv2d(decoder_features[-1], n_classes, kernel_size=1)

    def forward(self, x):
        bottleneck, skip_connections = self.encoder(x)
        x = self.decoder(bottleneck, skip_connections)
        return self.final_conv(x)


def test_residual_unet():
    print("=" * 60)
    print("          æ®‹å·®UNetæ¨¡å‹æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•é…ç½®
    batch_size = 2
    img_size = 256
    n_channels = 3
    n_classes = 2

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn((batch_size, n_channels, img_size, img_size))
    print(f"\nğŸ“Š æµ‹è¯•æ•°æ®ä¿¡æ¯:")
    print(f"  è¾“å…¥å°ºå¯¸: {x.shape}")
    print(f"  è¾“å…¥èŒƒå›´: [{x.min():.3f}, {x.max():.3f}]")

    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ”§ æ¨¡å‹é…ç½®:")
    print(f"  è¾“å…¥é€šé“: {n_channels}")
    print(f"  è¾“å‡ºç±»åˆ«: {n_classes}")
    print(f"  ç‰¹å¾é€šé“: [64, 128, 256, 512, 1024]")

    model = ResidualUNet(n_channels=n_channels, n_classes=n_classes)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\nğŸš€ å‰å‘ä¼ æ’­æµ‹è¯•:")
    model.eval()
    with torch.no_grad():
        output = model(x)

    print(f"  è¾“å‡ºå°ºå¯¸: {output.shape}")
    print(f"  è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")

    # è®¡ç®—å‚æ•°æ•°é‡
    print(f"\nğŸ“ˆ æ¨¡å‹ç»Ÿè®¡:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")

    # æµ‹è¯•ç¼–ç å™¨
    print(f"\nğŸ” ç¼–ç å™¨æµ‹è¯•:")
    bottleneck, skip_connections = model.encoder(x)
    print(f"  ç“¶é¢ˆå±‚è¾“å‡º: {bottleneck.shape}")
    for i, skip in enumerate(skip_connections):
        print(f"  è·³è·ƒè¿æ¥ {i}: {skip.shape}")

    # æµ‹è¯•è§£ç å™¨
    print(f"\nğŸ” è§£ç å™¨æµ‹è¯•:")
    decoder_output = model.decoder(bottleneck, skip_connections)
    print(f"  è§£ç å™¨è¾“å‡º: {decoder_output.shape}")

    # æµ‹è¯•æ®‹å·®å—
    print(f"\nğŸ” æ®‹å·®å—æµ‹è¯•:")
    residual_block = ResidualDoubleConv(64, 128)
    test_input = torch.randn(2, 64, 32, 32)
    residual_output = residual_block(test_input)
    print(f"  æ®‹å·®å—è¾“å…¥: {test_input.shape}")
    print(f"  æ®‹å·®å—è¾“å‡º: {residual_output.shape}")

    # å†…å­˜ä½¿ç”¨æµ‹è¯•
    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•:")
    if torch.cuda.is_available():
        device = torch.device('cuda')
        model_cuda = ResidualUNet(n_channels=n_channels, n_classes=n_classes).to(device)
        x_cuda = x.to(device)

        torch.cuda.synchronize()
        start_memory = torch.cuda.memory_allocated(device)

        with torch.no_grad():
            output_cuda = model_cuda(x_cuda)

        torch.cuda.synchronize()
        end_memory = torch.cuda.memory_allocated(device)
        memory_used = (end_memory - start_memory) / 1024 ** 2  # MB

        print(f"  GPUå†…å­˜ä½¿ç”¨: {memory_used:.2f} MB")
        print(f"  GPUè¾“å‡ºå°ºå¯¸: {output_cuda.shape}")
    else:
        print("  GPUä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")

    # æ¢¯åº¦æµ‹è¯•
    print(f"\nğŸ“‰ æ¢¯åº¦æµæµ‹è¯•:")
    model.train()
    x.requires_grad_(True)
    output = model(x)

    # åˆ›å»ºæ¨¡æ‹Ÿæ ‡ç­¾
    target = torch.randint(0, n_classes, (batch_size, img_size, img_size))

    # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
    criterion = nn.CrossEntropyLoss()
    loss = criterion(output, target)
    loss.backward()

    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
    print(f"  è¾“å…¥æ¢¯åº¦: {x.grad is not None}")

    # æ£€æŸ¥æ¨¡å‹ç»„ä»¶
    print(f"\nğŸ”§ æ¨¡å‹ç»„ä»¶æ£€æŸ¥:")
    print(f"  ç¼–ç å™¨å±‚æ•°: {len(model.encoder.encoder_layers)}")
    print(f"  è§£ç å™¨å±‚æ•°: {len(model.decoder.decoder_layers)}")
    print(f"  ä¸Šé‡‡æ ·å±‚æ•°: {len(model.decoder.up_convs)}")

    # æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸
    print(f"\nğŸ“ ä¸åŒè¾“å…¥å°ºå¯¸æµ‹è¯•:")
    test_sizes = [128, 256, 512]
    for size in test_sizes:
        test_x = torch.randn(1, n_channels, size, size)
        with torch.no_grad():
            test_output = model(test_x)
        print(f"  è¾“å…¥ {size}x{size} -> è¾“å‡º {test_output.shape[2]}x{test_output.shape[3]}")

    return output, model


def test_residual_connections():
    """ä¸“é—¨æµ‹è¯•æ®‹å·®è¿æ¥çš„åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("          æ®‹å·®è¿æ¥ä¸“é¡¹æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•1: ç›¸åŒé€šé“æ•°çš„æ®‹å·®å—
    print("\n1. ç›¸åŒé€šé“æ•°æ®‹å·®å—æµ‹è¯•:")
    block_same = ResidualDoubleConv(64, 64)
    x_same = torch.randn(2, 64, 16, 16)
    out_same = block_same(x_same)
    print(f"   è¾“å…¥: {x_same.shape}, è¾“å‡º: {out_same.shape}")
    print(f"   æ˜¯å¦ä½¿ç”¨shortcutå·ç§¯: {block_same.use_shortcut}")

    # æµ‹è¯•2: ä¸åŒé€šé“æ•°çš„æ®‹å·®å—
    print("\n2. ä¸åŒé€šé“æ•°æ®‹å·®å—æµ‹è¯•:")
    block_diff = ResidualDoubleConv(64, 128)
    x_diff = torch.randn(2, 64, 16, 16)
    out_diff = block_diff(x_diff)
    print(f"   è¾“å…¥: {x_diff.shape}, è¾“å‡º: {out_diff.shape}")
    print(f"   æ˜¯å¦ä½¿ç”¨shortcutå·ç§¯: {block_diff.use_shortcut}")

    # æµ‹è¯•3: éªŒè¯æ®‹å·®è¿æ¥ç¡®å®å­˜åœ¨
    print("\n3. æ®‹å·®è¿æ¥éªŒè¯:")
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•ï¼Œç¡®ä¿è¾“å‡ºä¸æ˜¯æ’ç­‰æ˜ å°„
    test_input = torch.ones(1, 32, 8, 8) * 0.5
    test_block = ResidualDoubleConv(32, 32)
    test_output = test_block(test_input)

    # å¦‚æœæ®‹å·®è¿æ¥å·¥ä½œæ­£å¸¸ï¼Œè¾“å‡ºåº”è¯¥ä¸è¾“å…¥ä¸åŒ
    is_different = not torch.allclose(test_input, test_output, atol=1e-6)
    print(f"   è¾“å…¥è¾“å‡ºæ˜¯å¦ä¸åŒ: {is_different}")
    print(f"   è¾“å…¥å‡å€¼: {test_input.mean():.4f}")
    print(f"   è¾“å‡ºå‡å€¼: {test_output.mean():.4f}")


if __name__ == "__main__":
    try:
        # è¿è¡Œæ®‹å·®è¿æ¥ä¸“é¡¹æµ‹è¯•
        test_residual_connections()

        # è¿è¡Œå®Œæ•´æ¨¡å‹æµ‹è¯•
        output, model = test_residual_unet()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ®‹å·®UNetå®ç°æˆåŠŸï¼")
        print("=" * 60)
        print(f"âœ… æ¨¡å‹è¾“å‡ºå°ºå¯¸: {output.shape}")
        print(f"âœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"âœ… æ®‹å·®è¿æ¥æ­£å¸¸å·¥ä½œ")
        print(f"âœ… æ¢¯åº¦æµæ­£å¸¸")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()