import torch
import torch.optim as optim
import wandb  # å¯¼å…¥wandb
from .metrics import SegmentationMetrics, MetricTracker


class UNetTrainer:
    def __init__(self, model, device, train_loader, val_loader=None, experiment=None, save_dir='run'):
        self.model = model
        self.device = device
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.metrics_calculator = SegmentationMetrics()
        self.model.to(self.device)
        self.experiment = experiment  # WandBå®éªŒå¯¹è±¡
        self.global_step = 0  # å…¨å±€æ­¥æ•°è®¡æ•°
        self.save_dir = save_dir  # æ–°å¢ä¿å­˜ç›®å½•
    
    def train_epoch(self, optimizer, criterion, cur_epoch, scheduler=None):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        tracker = MetricTracker()
        epoch_loss = 0

        # å®šæœŸè®°å½•å›¾åƒç¤ºä¾‹ï¼ˆæ¯10ä¸ªbatchï¼‰
        log_images = True

        for batch_idx, (images, masks) in enumerate(self.train_loader):
            images, masks = images.to(self.device), masks.to(self.device)
            self.global_step += 1

            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = self.model(images)

            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, masks)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
            batch_metrics = self.metrics_calculator.calculate_all_metrics(outputs, masks)
            tracker.update(batch_metrics, loss.item())

            epoch_loss += loss.item()

            # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°WandB
            if self.experiment:
                # è®°å½•æ¯ä¸ªbatchçš„æŸå¤±å’Œå­¦ä¹ ç‡
                self.experiment.log({
                    'train/loss': loss.item(),
                    'train/accuracy': batch_metrics['accuracy'],
                    'train/dice': batch_metrics['dice'],
                    'train/mIoU': batch_metrics['miou'],
                    'learning_rate': optimizer.param_groups[0]['lr'],
                    'step': self.global_step,
                    'epoch': cur_epoch
                })
                

                # å®šæœŸè®°å½•å›¾åƒç¤ºä¾‹
                # if log_images and batch_idx % 10 == 0:
                #     # å¤„ç†è¾“å‡ºä»¥å¯è§†åŒ–ï¼ˆå¦‚æœæ˜¯ sigmoid è¾“å‡ºï¼Œè½¬æ¢ä¸ºæ¦‚ç‡ï¼‰
                #     preds = torch.sigmoid(outputs) if outputs.shape[1] == 1 else torch.softmax(outputs, dim=1)
                #     pred_masks = (preds > 0.5).float()  # äºŒå€¼åŒ–

                #     # è®°å½•å›¾åƒã€çœŸå®æ©ç å’Œé¢„æµ‹æ©ç 
                #     self.experiment.log({
                #         'train/sample_images': wandb.Image(
                #             images[0].cpu(),
                #             masks={
                #                 "true": wandb.Image(masks[0].cpu()),
                #                 "pred": wandb.Image(pred_masks[0].cpu())
                #             },
                #             caption=f"Train Epoch {cur_epoch}, Batch {batch_idx}"
                #         )
                #     })
                #     log_images = False  # æ¯ä¸ªepochåªè®°å½•ä¸€æ¬¡å›¾åƒ

            if batch_idx % 10 == 0:
                current_metrics = tracker.average()
                print(f'Batch {batch_idx}/{len(self.train_loader)}, '
                      f'Loss: {current_metrics["loss"]:.4f}, '
                      f'Acc: {current_metrics["accuracy"]:.4f}')

        avg_metrics = tracker.average()
        return avg_metrics

    def validate(self, criterion, epoch):
        """éªŒè¯å¹¶è®°å½•æŒ‡æ ‡åˆ°WandB"""
        if self.val_loader is None:
            return None

        self.model.eval()
        tracker = MetricTracker()
        log_images = True  # è®°å½•éªŒè¯å›¾åƒ

        with torch.no_grad():
            for batch_idx, (images, masks) in enumerate(self.val_loader):
                images, masks = images.to(self.device), masks.to(self.device)
                outputs = self.model(images)

                loss = criterion(outputs, masks)
                batch_metrics = self.metrics_calculator.calculate_all_metrics(outputs, masks)
                tracker.update(batch_metrics, loss.item())

                # è®°å½•éªŒè¯å›¾åƒ
                # if self.experiment and log_images:
                #     preds = torch.sigmoid(outputs) if outputs.shape[1] == 1 else torch.softmax(outputs, dim=1)
                #     pred_masks = (preds > 0.5).float()

                #     self.experiment.log({
                #         'val/sample_images': wandb.Image(
                #             images[0].cpu(),
                #             masks={
                #                 "true": wandb.Image(masks[0].cpu()),
                #                 "pred": wandb.Image(pred_masks[0].cpu())
                #             },
                #             caption=f"Val Epoch {epoch}"
                #         )
                #     })
                #     log_images = False

        val_metrics = tracker.average()

        # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°WandB
        if self.experiment:
            self.experiment.log({
                'val/loss': val_metrics['loss'],
                'val/accuracy': val_metrics['accuracy'],
                'val/dice': val_metrics['dice'],
                'val/mIoU': val_metrics['miou'],
                'val/precision': val_metrics['precision'],
                'val/recall': val_metrics['recall'],
                'val/f1': val_metrics['f1'],
                'epoch': epoch
            })

        return val_metrics

    def train(self, epochs, optimizer, criterion, scheduler=None, save_path='best_model.pth'):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        best_val_loss = float('inf')
        train_history = {
            'epoch': [],
            'train_loss': [], 'train_iou': [], 'train_dice': [], 'train_precision': [],
            'train_recall': [], 'train_f1': [], 'train_accuracy': [], 'train_miou': [],
            'val_loss': [], 'val_iou': [], 'val_dice': [], 'val_precision': [],
            'val_recall': [], 'val_f1': [], 'val_accuracy': [], 'val_miou': []
        }

        for epoch in range(epochs):
            print(f'\nEpoch {epoch + 1}/{epochs}')
            print('-' * 60)

            # è®­ç»ƒ
            train_metrics = self.train_epoch(optimizer, criterion, epoch, scheduler)

            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            train_history['epoch'].append(epoch + 1)
            for key in train_metrics:
                train_history[f'train_{key}'].append(train_metrics[key])

            print(f"Train - {self._format_metrics(train_metrics)}")

            # è®°å½• epoch çº§åˆ«çš„è®­ç»ƒæŒ‡æ ‡
            if self.experiment:
                self.experiment.log({
                    'train/epoch_loss': train_metrics['loss'],
                    'train/epoch_accuracy': train_metrics['accuracy'],
                    'train/epoch_mIoU': train_metrics['miou'],
                    'train/epoch_dice': train_metrics['dice'],
                    'epoch': epoch + 1
                })

            # éªŒè¯
            if self.val_loader is not None:
                val_metrics = self.validate(criterion, epoch + 1)

                # è®°å½•éªŒè¯æŒ‡æ ‡
                for key in val_metrics:
                    train_history[f'val_{key}'].append(val_metrics[key])

                print(f"Val   - {self._format_metrics(val_metrics)}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['loss'] < best_val_loss:
                    best_val_loss = val_metrics['loss']
                    torch.save(self.model.state_dict(), save_path)
                    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path} (Loss: {best_val_loss:.4f})")
                    if self.experiment:
                        self.experiment.log({"best_val_loss": best_val_loss})

            # å­¦ä¹ ç‡è°ƒåº¦
            if scheduler is not None:
                if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau) and self.val_loader is not None:
                    scheduler.step(val_metrics['loss'])
                else:
                    scheduler.step()

            # æ¯10ä¸ªepochæ‰“å°è¯¦ç»†æŒ‡æ ‡
            if (epoch + 1) % 10 == 0:
                self._print_detailed_metrics(train_metrics, val_metrics if self.val_loader else None)

        return train_history

    def _format_metrics(self, metrics: dict) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡è¾“å‡º"""
        return (f"Loss: {metrics['loss']:.4f} | "
                f"Acc: {metrics['accuracy']:.4f} | "
                f"mIoU: {metrics['miou']:.4f} | "
                f"Dice: {metrics['dice']:.4f} | "
                f"F1: {metrics['f1']:.4f}")

    def _print_detailed_metrics(self, train_metrics: dict, val_metrics: dict = None):
        """æ‰“å°è¯¦ç»†æŒ‡æ ‡"""
        print("\n" + "=" * 80)
        print("ğŸ“Š è¯¦ç»†æŒ‡æ ‡æŠ¥å‘Š")
        print("=" * 80)

        headers = ["Metric", "Train"]
        if val_metrics:
            headers.append("Val")

        print(f"{'Metric':<12} {'Train':<10} {'Val':<10}" if val_metrics else f"{'Metric':<12} {'Train':<10}")
        print("-" * (35 if val_metrics else 25))

        metrics_list = [
            ('Loss', 'loss'), ('Accuracy', 'accuracy'), ('mIoU', 'miou'),
            ('Dice', 'dice'), ('IoU', 'iou'), ('F1', 'f1'),
            ('Precision', 'precision'), ('Recall', 'recall')
        ]

        for name, key in metrics_list:
            if val_metrics:
                print(f"{name:<12} {train_metrics[key]:.4f}     {val_metrics[key]:.4f}")
            else:
                print(f"{name:<12} {train_metrics[key]:.4f}")

        print("=" * 80)


def get_optimizer(model, optimizer_name='adam', learning_rate=1e-4, weight_decay=1e-5):
    """è·å–ä¼˜åŒ–å™¨"""
    if optimizer_name.lower() == 'adam':
        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'adamw':
        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'sgd':
        return optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")


def get_scheduler(optimizer, scheduler_name='step', **kwargs):
    """è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if scheduler_name == 'step':
        return optim.lr_scheduler.StepLR(optimizer, step_size=kwargs.get('step_size', 30),
                                         gamma=kwargs.get('gamma', 0.1))
    elif scheduler_name == 'plateau':
        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=kwargs.get('patience', 10),
                                                    factor=kwargs.get('factor', 0.5))
    elif scheduler_name == 'cosine':
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=kwargs.get('T_max', 50))
    else:
        return None